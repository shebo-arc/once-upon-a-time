{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerStoryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
    "        memory = self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=src_padding_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def encode(self, src):\n",
    "        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n",
    "        src_padding_mask = (src == 0)\n",
    "        return self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, memory):\n",
    "        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# Tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, tokenization_type='word'):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.tokenization_type = tokenization_type\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.bos_token = '[BOS]'\n",
    "        self.eos_token = '[EOS]'\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.word_to_idx = {self.pad_token: 0, self.unk_token: 1, self.bos_token: 2, self.eos_token: 3}\n",
    "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split() if self.tokenization_type == 'word' else list(text)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(self.tokenize(text))\n",
    "        for token in sorted(vocab):\n",
    "            if token not in self.word_to_idx:\n",
    "                self.word_to_idx[token] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = token\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = self.tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        return [self.word_to_idx.get(token, self.word_to_idx[self.unk_token]) for token in tokens]\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        tokens = [self.idx_to_word.get(id, self.unk_token) for id in ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]]\n",
    "        return ' '.join(tokens) if self.tokenization_type == 'word' else ''.join(tokens)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "# Dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, seq_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenized_stories = []\n",
    "        for story in stories:\n",
    "            tokens = tokenizer.encode(story, add_special_tokens=True)\n",
    "            self.tokenized_stories.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_stories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenized_stories[idx]\n",
    "        if len(tokens) <= self.seq_length + 1:\n",
    "            tokens = tokens + [0] * (self.seq_length + 1 - len(tokens))\n",
    "        else:\n",
    "            start = random.randint(0, len(tokens) - self.seq_length - 1)\n",
    "            tokens = tokens[start:start + self.seq_length + 1]\n",
    "        src = torch.tensor(tokens[:-1])\n",
    "        tgt = torch.tensor(tokens[1:])\n",
    "        return src, tgt\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=10):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for src, tgt in progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            output_flat = output.view(-1, model.vocab_size)\n",
    "            target_flat = tgt.contiguous().view(-1)\n",
    "            loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in progress_bar:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                output = model(src, tgt)\n",
    "                output_flat = output.view(-1, model.vocab_size)\n",
    "                target_flat = tgt.contiguous().view(-1)\n",
    "                loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "                val_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Text generation\n",
    "def generate_story(model, tokenizer, prompt, max_length=100, temperature=1.0, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    output_ids = input_ids.copy()\n",
    "    for _ in range(max_length):\n",
    "        curr_input = torch.tensor([output_ids[-min(len(output_ids), model.d_model):]], dtype=torch.long).to(device)\n",
    "        memory = model.encode(curr_input)\n",
    "        tgt_input = torch.tensor([[output_ids[-1]]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_input, memory)\n",
    "        logits = output[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, 1).item()\n",
    "        output_ids.append(next_token_id)\n",
    "        if next_token_id == tokenizer.word_to_idx[tokenizer.eos_token]:\n",
    "            break\n",
    "    return tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading TinyStories dataset...\n",
      "Fitting tokenizer on dataset...\n",
      "Vocabulary size: 76887\n",
      "Adjusted batch_size: 32, Estimated steps: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   0%|                                                                      | 0/2500 [00:00<?, ?it/s]E:\\Supporting Tools\\Python\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/5 [Train]:   5%|██▎                                            | 120/2500 [05:41<1:52:53,  2.85s/it, loss=5.07]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[0;32m     70\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_model\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m#\"/kaggle/working/cusLLM_model\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    152\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 153\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[0;32m    155\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyStories dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(tokenization_type='word')\n",
    "\n",
    "# Fit tokenizer on a larger subset of the data\n",
    "print(\"Fitting tokenizer on dataset...\")\n",
    "sample_size = 100000  # Use a larger sample for robust vocabulary\n",
    "sample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"])]\n",
    "tokenizer.fit(sample_texts)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "seq_length = 128\n",
    "# Create a single dataset from a larger subset of training data\n",
    "max_samples = 100000  # Use 50,000 stories to ensure enough sequences\n",
    "stories = [ds[\"train\"][i][\"text\"] for i in range(min(max_samples, len(ds[\"train\"])))]\n",
    "dataset = StoryDataset(stories, tokenizer, seq_length=seq_length)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Calculate required batch size for ~1000 steps\n",
    "target_steps = 1000\n",
    "estimated_dataset_size = len(train_dataset)\n",
    "batch_size = max(1, estimated_dataset_size // target_steps)  # Ensure at least 1\n",
    "if batch_size > 32:  # Cap batch size to avoid memory issues\n",
    "    batch_size = 32\n",
    "print(f\"Adjusted batch_size: {batch_size}, Estimated steps: {len(train_dataset) // batch_size}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# Create model\n",
    "model = TransformerStoryModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,      # Embedding dimension\n",
    "    nhead=8,          # Number of attention heads\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"simple_model\"     #\"/kaggle/working/cusLLM_model\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": {\n",
    "        \"word_to_idx\": tokenizer.word_to_idx,\n",
    "        \"idx_to_word\": tokenizer.idx_to_word,\n",
    "        \"tokenization_type\": tokenizer.tokenization_type\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "    }\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyStories dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(tokenization_type='word')\n",
    "\n",
    "# Fit tokenizer on a larger subset of the data\n",
    "print(\"Fitting tokenizer on dataset...\")\n",
    "sample_size = 100000  # Use a larger sample for robust vocabulary\n",
    "sample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"])]\n",
    "tokenizer.fit(sample_texts)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create model\n",
    "model = TransformerStoryModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,      # Embedding dimension\n",
    "    nhead=8,          # Number of attention heads\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# Load the best model for text generation\n",
    "best_model_path = \"simple_model/final_model.pt\"    #\"/kaggle/working/cusLLM_model/best_model.pt\"\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}\")\n",
    "\n",
    "# Get user input for text generation\n",
    "prompt = input(\"Enter a prompt for text generation: \")\n",
    "generated_text = generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    temperature=0.8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
