{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6fce93ab-c738-4610-90ad-95479adb5050",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb18c9f6-4926-4863-8d71-a07e168e912e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in e:\\supporting tools\\python\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in e:\\supporting tools\\python\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in e:\\supporting tools\\python\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in e:\\supporting tools\\python\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\supporting tools\\python\\lib\\site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: networkx in e:\\supporting tools\\python\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\supporting tools\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in e:\\supporting tools\\python\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: setuptools in e:\\supporting tools\\python\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\supporting tools\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\supporting tools\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in e:\\supporting tools\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\supporting tools\\python\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\supporting tools\\python\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\supporting tools\\python\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\supporting tools\\python\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\supporting tools\\python\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\supporting tools\\python\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\supporting tools\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in e:\\supporting tools\\python\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in e:\\supporting tools\\python\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\supporting tools\\python\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\supporting tools\\python\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\supporting tools\\python\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\supporting tools\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\supporting tools\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\supporting tools\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\supporting tools\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.6.2)\n",
      "Requirement already satisfied: colorama in e:\\supporting tools\\python\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\supporting tools\\python\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\supporting tools\\python\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\supporting tools\\python\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\supporting tools\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install scikit-learn\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb5decb-4002-47ad-9809-f4f9c08638f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f81fd6-fade-4cce-ab1c-312a2f902ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b2d709-7da0-439f-a273-b6e6093d9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading TinyStories dataset...\n",
      "Fitting tokenizer on dataset...\n",
      "Vocabulary size: 10698\n",
      "Tokenizing train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 5936.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 6418.37it/s]\n",
      "Epoch 1/10 [Train]:   0%|                                                                      | 0/313 [00:00<?, ?it/s]E:\\Supporting Tools\\Python\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|█████████████████████████████████████████████████| 313/313 [11:18<00:00,  2.17s/it, loss=5.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training loss: 6.2434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Val]:   0%|                                                                        | 0/313 [00:00<?, ?it/s]E:\\Supporting Tools\\Python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Epoch 1/10 [Val]: 100%|███████████████████████████████████████████████████| 313/313 [05:20<00:00,  1.02s/it, loss=4.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation loss: 5.1652\n",
      "Saved new best model with validation loss: 5.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]:   1%|▍                                                   | 3/313 [00:14<25:13,  4.88s/it, loss=5.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 423\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 376\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    373\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_custom_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your needs\u001b[39;49;00m\n\u001b[0;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[0;32m    387\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 239\u001b[0m, in \u001b[0;36mtrain_custom_model\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[0;32m    236\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Update loss\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n\u001b[0;32m    242\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Custom Transformer implementation\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter but part of the module)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer architecture\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                  dim_feedforward=dim_feedforward, \n",
    "                                                  dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                  dim_feedforward=dim_feedforward,\n",
    "                                                  dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        \n",
    "        # Create padding masks\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        \n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Embedding and positional encoding\n",
    "        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
    "        \n",
    "        # Transformer encoding and decoding\n",
    "        memory = self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, \n",
    "                                         tgt_mask=tgt_mask,\n",
    "                                         tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                         memory_key_padding_mask=src_padding_mask)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        return self.output_layer(output)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n",
    "        src_padding_mask = (src == 0)\n",
    "        return self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "    \n",
    "    def decode(self, tgt, memory):\n",
    "        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        \n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# Custom tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, tokenization_type='word'):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.tokenization_type = tokenization_type  # 'word' or 'char'\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.bos_token = '[BOS]'\n",
    "        self.eos_token = '[EOS]'\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.add_special_tokens()\n",
    "    \n",
    "    def add_special_tokens(self):\n",
    "        self.word_to_idx = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3\n",
    "        }\n",
    "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        if self.tokenization_type == 'word':\n",
    "            return text.split()\n",
    "        else:  # char tokenization\n",
    "            return list(text)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        vocab = set()\n",
    "        \n",
    "        # Extract all unique tokens\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            vocab.update(tokens)\n",
    "        \n",
    "        # Add tokens to vocabulary\n",
    "        for token in sorted(vocab):\n",
    "            if token not in self.word_to_idx:\n",
    "                self.word_to_idx[token] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = token\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        \n",
    "        # Convert tokens to IDs\n",
    "        ids = [self.word_to_idx.get(token, self.word_to_idx[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        tokens = [self.idx_to_word.get(id, self.unk_token) for id in ids]\n",
    "        \n",
    "        # Remove special tokens if needed\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]]\n",
    "        \n",
    "        # Join tokens\n",
    "        if self.tokenization_type == 'word':\n",
    "            return ' '.join(tokens)\n",
    "        else:  # char tokenization\n",
    "            return ''.join(tokens)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "# Dataset for text generation using TinyStories\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, seq_length=64, split=\"train\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.dataset = dataset[split]\n",
    "        \n",
    "        # Tokenize the stories\n",
    "        print(f\"Tokenizing {split} dataset...\")\n",
    "        self.tokenized_stories = []\n",
    "        \n",
    "        # Process a subset for faster training (adjust as needed)\n",
    "        num_samples = min(10000, len(self.dataset))\n",
    "        for i in tqdm(range(num_samples)):\n",
    "            story = self.dataset[i][\"text\"]\n",
    "            tokens = tokenizer.encode(story, add_special_tokens=True)\n",
    "            self.tokenized_stories.append(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_stories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenized_stories[idx]\n",
    "        \n",
    "        # Ensure tokens are the right length\n",
    "        if len(tokens) <= self.seq_length + 1:\n",
    "            # Pad to sequence length\n",
    "            tokens = tokens + [0] * (self.seq_length + 1 - len(tokens))\n",
    "        else:\n",
    "            # Choose a random starting point to fit sequence length\n",
    "            start = random.randint(0, len(tokens) - self.seq_length - 1)\n",
    "            tokens = tokens[start:start + self.seq_length + 1]\n",
    "        \n",
    "        src = torch.tensor(tokens[:-1])\n",
    "        tgt = torch.tensor(tokens[1:])\n",
    "        \n",
    "        return src, tgt\n",
    "\n",
    "# Training function\n",
    "def train_custom_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs=10):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(progress_bar):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, src)  # Using teacher forcing\n",
    "            \n",
    "            # Reshape output and target for loss calculation\n",
    "            output_flat = output.view(-1, model.vocab_size)\n",
    "            target_flat = tgt.contiguous().view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (src, tgt) in enumerate(progress_bar):\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(src, src)\n",
    "                \n",
    "                # Calculate loss\n",
    "                output_flat = output.view(-1, model.vocab_size)\n",
    "                target_flat = tgt.contiguous().view(-1)\n",
    "                loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "                \n",
    "                # Update loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": best_val_loss,\n",
    "            }, \"best_model.pt\")\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Text generation function\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize output sequence\n",
    "    output_ids = input_ids.copy()\n",
    "    \n",
    "    # Generate one token at a time\n",
    "    for _ in range(max_length):\n",
    "        # Prepare input (truncate if too long)\n",
    "        curr_input = torch.tensor([output_ids[-min(len(output_ids), model.d_model):]], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate memory from encoder\n",
    "        memory = model.encode(curr_input)\n",
    "        \n",
    "        # Create target input (last token)\n",
    "        tgt_input = torch.tensor([[output_ids[-1]]], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_input, memory)\n",
    "            \n",
    "        # Apply temperature and get probabilities\n",
    "        logits = output[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        next_token_id = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        # Add to output sequence\n",
    "        output_ids.append(next_token_id)\n",
    "        \n",
    "        # Stop if end of sequence\n",
    "        if next_token_id == tokenizer.word_to_idx[tokenizer.eos_token]:\n",
    "            break\n",
    "    \n",
    "    # Decode output sequence\n",
    "    generated_text = tokenizer.decode(output_ids)\n",
    "    return generated_text\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load TinyStories dataset\n",
    "    print(\"Loading TinyStories dataset...\")\n",
    "    ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = SimpleTokenizer(tokenization_type='word')\n",
    "    \n",
    "    # Fit tokenizer on a subset of the data\n",
    "    print(\"Fitting tokenizer on dataset...\")\n",
    "    sample_size = 1000  # Adjust based on your needs\n",
    "    sample_texts = [ds[\"train\"][i][\"text\"] for i in range(sample_size)]\n",
    "    tokenizer.fit(sample_texts)\n",
    "    \n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    seq_length = 128\n",
    "    train_dataset = TinyStoriesDataset(ds, tokenizer, seq_length=seq_length, split=\"train\")\n",
    "    val_dataset = TinyStoriesDataset(ds, tokenizer, seq_length=seq_length, split=\"validation\")\n",
    "    \n",
    "    batch_size = 32  # Adjust based on your GPU memory\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Create model\n",
    "    model = CustomTransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=256,      # Embedding dimension\n",
    "        nhead=8,          # Number of attention heads\n",
    "        num_encoder_layers=4,\n",
    "        num_decoder_layers=4,\n",
    "        dim_feedforward=1024\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_custom_model(\n",
    "        model, \n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epochs=10  # Adjust based on your needs\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load(\"best_model.pt\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"Once upon a time\"\n",
    "    generated_text = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_length=200,\n",
    "        temperature=0.8,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated text:\\n{generated_text}\")\n",
    "    \n",
    "    # Save final model\n",
    "    output_dir = \"./tinystories_model\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"tokenizer\": {\n",
    "            \"word_to_idx\": tokenizer.word_to_idx,\n",
    "            \"idx_to_word\": tokenizer.idx_to_word,\n",
    "            \"tokenization_type\": tokenizer.tokenization_type\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"d_model\": model.d_model,\n",
    "            \"vocab_size\": model.vocab_size,\n",
    "        }\n",
    "    }, f\"{output_dir}/model.pt\")\n",
    "    print(f\"Model saved to {output_dir}/model.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50605f-fc27-4077-aa18-c6030ab421cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
