{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport os\nfrom datasets import load_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:05:07.362710Z","iopub.execute_input":"2025-04-19T16:05:07.362870Z","iopub.status.idle":"2025-04-19T16:05:12.807523Z","shell.execute_reply.started":"2025-04-19T16:05:07.362855Z","shell.execute_reply":"2025-04-19T16:05:12.806771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length=5000):\n        super().__init__()\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# Transformer Model\nclass TransformerStoryModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        \n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n\n    def create_mask(self, src, tgt):\n        src_seq_len = src.shape[1]\n        tgt_seq_len = tgt.shape[1]\n        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n        src_padding_mask = (src == 0)\n        tgt_padding_mask = (tgt == 0)\n        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, src, tgt):\n        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n        memory = self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=src_padding_mask)\n        return self.output_layer(output)\n\n    def encode(self, src):\n        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n        src_padding_mask = (src == 0)\n        return self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n\n    def decode(self, tgt, memory):\n        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n        tgt_padding_mask = (tgt == 0)\n        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask)\n        return self.output_layer(output)\n\n# Tokenizer\nclass SimpleTokenizer:\n    def __init__(self, tokenization_type='word'):\n        self.word_to_idx = {}\n        self.idx_to_word = {}\n        self.tokenization_type = tokenization_type\n        self.pad_token = '[PAD]'\n        self.unk_token = '[UNK]'\n        self.bos_token = '[BOS]'\n        self.eos_token = '[EOS]'\n        self.add_special_tokens()\n\n    def add_special_tokens(self):\n        self.word_to_idx = {self.pad_token: 0, self.unk_token: 1, self.bos_token: 2, self.eos_token: 3}\n        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n\n    def tokenize(self, text):\n        return text.split() if self.tokenization_type == 'word' else list(text)\n\n    def fit(self, texts):\n        vocab = set()\n        for text in texts:\n            vocab.update(self.tokenize(text))\n        for token in sorted(vocab):\n            if token not in self.word_to_idx:\n                self.word_to_idx[token] = len(self.word_to_idx)\n                self.idx_to_word[len(self.idx_to_word)] = token\n\n    def encode(self, text, add_special_tokens=True):\n        tokens = self.tokenize(text)\n        if add_special_tokens:\n            tokens = [self.bos_token] + tokens + [self.eos_token]\n        return [self.word_to_idx.get(token, self.word_to_idx[self.unk_token]) for token in tokens]\n\n    def decode(self, ids, skip_special_tokens=True):\n        tokens = [self.idx_to_word.get(id, self.unk_token) for id in ids]\n        if skip_special_tokens:\n            tokens = [token for token in tokens if token not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]]\n        return ' '.join(tokens) if self.tokenization_type == 'word' else ''.join(tokens)\n\n    def vocab_size(self):\n        return len(self.word_to_idx)\n\n# Dataset\nclass StoryDataset(Dataset):\n    def __init__(self, stories, tokenizer, seq_length=64):\n        self.tokenizer = tokenizer\n        self.seq_length = seq_length\n        self.tokenized_stories = []\n        for story in stories:\n            tokens = tokenizer.encode(story, add_special_tokens=True)\n            self.tokenized_stories.append(tokens)\n\n    def __len__(self):\n        return len(self.tokenized_stories)\n\n    def __getitem__(self, idx):\n        tokens = self.tokenized_stories[idx]\n        if len(tokens) <= self.seq_length + 1:\n            tokens = tokens + [0] * (self.seq_length + 1 - len(tokens))\n        else:\n            start = random.randint(0, len(tokens) - self.seq_length - 1)\n            tokens = tokens[start:start + self.seq_length + 1]\n        src = torch.tensor(tokens[:-1])\n        tgt = torch.tensor(tokens[1:])\n        return src, tgt\n\n# Training function\ndef train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=10):\n    model.train()\n    best_val_loss = float('inf')\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        for src, tgt in progress_bar:\n            src, tgt = src.to(device), tgt.to(device)\n            optimizer.zero_grad()\n            output = model(src, tgt)\n            output_flat = output.view(-1, model.vocab_size)\n            target_flat = tgt.contiguous().view(-1)\n            loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            epoch_loss += loss.item()\n            progress_bar.set_postfix({\"loss\": loss.item()})\n        avg_train_loss = epoch_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss:.4f}\")\n\n        model.eval()\n        val_loss = 0\n        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n        with torch.no_grad():\n            for src, tgt in progress_bar:\n                src, tgt = src.to(device), tgt.to(device)\n                output = model(src, tgt)\n                output_flat = output.view(-1, model.vocab_size)\n                target_flat = tgt.contiguous().view(-1)\n                loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n                val_loss += loss.item()\n                progress_bar.set_postfix({\"loss\": loss.item()})\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss:.4f}\")\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n    return model\n\n# Text generation\ndef generate_story(model, tokenizer, prompt, max_length=100, temperature=1.0, device=\"cuda\"):\n    model.eval()\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n    output_ids = input_ids.copy()\n    for _ in range(max_length):\n        curr_input = torch.tensor([output_ids[-min(len(output_ids), model.d_model):]], dtype=torch.long).to(device)\n        memory = model.encode(curr_input)\n        tgt_input = torch.tensor([[output_ids[-1]]], dtype=torch.long).to(device)\n        with torch.no_grad():\n            output = model.decode(tgt_input, memory)\n        logits = output[0, -1, :] / temperature\n        probs = F.softmax(logits, dim=-1)\n        next_token_id = torch.multinomial(probs, 1).item()\n        output_ids.append(next_token_id)\n        if next_token_id == tokenizer.word_to_idx[tokenizer.eos_token]:\n            break\n    return tokenizer.decode(output_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:07:10.300275Z","iopub.execute_input":"2025-04-19T16:07:10.300983Z","iopub.status.idle":"2025-04-19T16:07:10.335531Z","shell.execute_reply.started":"2025-04-19T16:07:10.300959Z","shell.execute_reply":"2025-04-19T16:07:10.334693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load TinyStories dataset\nprint(\"Loading TinyStories dataset...\")\nds = load_dataset(\"roneneldan/TinyStories\")\n\n# Create tokenizer\ntokenizer = SimpleTokenizer(tokenization_type='word')\n\n# Fit tokenizer on a larger subset of the data\nprint(\"Fitting tokenizer on dataset...\")\nsample_size = 100000  # Use a larger sample for robust vocabulary\nsample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"])]\ntokenizer.fit(sample_texts)\n\nvocab_size = tokenizer.vocab_size()\nprint(f\"Vocabulary size: {vocab_size}\")\n\n# Create datasets and dataloaders\nseq_length = 128\n# Create a single dataset from a larger subset of training data\nmax_samples = 100000  # Use 50,000 stories to ensure enough sequences\nstories = [ds[\"train\"][i][\"text\"] for i in range(min(max_samples, len(ds[\"train\"])))]\ndataset = StoryDataset(stories, tokenizer, seq_length=seq_length)\n\n# Split the dataset into train and validation sets\ntrain_size = int(0.8 * len(dataset))  # 80% for training\nval_size = len(dataset) - train_size  # 20% for validation\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# Calculate required batch size for ~1000 steps\ntarget_steps = 1000\nestimated_dataset_size = len(train_dataset)\nbatch_size = max(1, estimated_dataset_size // target_steps)  # Ensure at least 1\nif batch_size > 32:  # Cap batch size to avoid memory issues\n    batch_size = 32\nprint(f\"Adjusted batch_size: {batch_size}, Estimated steps: {len(train_dataset) // batch_size}\")\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False)\n\n# Create model\nmodel = TransformerStoryModel(\n    vocab_size=vocab_size,\n    d_model=256,      # Embedding dimension\n    nhead=8,          # Number of attention heads\n    num_encoder_layers=6,\n    num_decoder_layers=6,\n    dim_feedforward=1024\n).to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n\n# Train model\ntrained_model = train_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    optimizer,\n    scheduler,\n    device,\n    epochs=5\n)\n\n# Save the final model\noutput_dir = \"/kaggle/working/cusLLM_model\"\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nfinal_model_path = os.path.join(output_dir, \"final_model.pt\")\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"tokenizer\": {\n        \"word_to_idx\": tokenizer.word_to_idx,\n        \"idx_to_word\": tokenizer.idx_to_word,\n        \"tokenization_type\": tokenizer.tokenization_type\n    },\n    \"config\": {\n        \"d_model\": model.d_model,\n        \"vocab_size\": model.vocab_size,\n    }\n}, final_model_path)\nprint(f\"Final model saved to {final_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:58:13.277970Z","iopub.execute_input":"2025-04-19T16:58:13.278603Z","iopub.status.idle":"2025-04-19T17:46:21.040605Z","shell.execute_reply.started":"2025-04-19T16:58:13.278576Z","shell.execute_reply":"2025-04-19T17:46:21.039817Z"}},"outputs":[],"execution_count":null}]}